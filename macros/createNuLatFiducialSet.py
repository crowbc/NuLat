import os
import sys
import time as timer
import random
import json
import math
import pandas as pd
import numpy as np

class createNuLatFiducialSet():
    """
    CreateNuLatFiducialSet class for use on processed RATPAC2 output files for NuLat reference data to be binned for FND fits.
    This code reads the ASCII data files generated by process_fiducial_data.sh, and bins the events in (normalized) 
    directional matrices based on the relative displacement between the positron annihilation (prompt) and 
    neutron capture (delayed) events within the segmented geometry.
    """
    ################################################################
    # __init__ method --- line 20
    ################################################################
    def __init__(self):
        """
        Initializes class and creates an instance with defined parameters
        Args:
            None.
        Return values:
            None.
        """
        # control Booleans
        self.debug = True
        self.suppress_output = True
        self.time_start = timer.time() # only use when self.debug == True
        # debugging message
        if(self.debug):
            print("CreateNuLatFiducialSet class initialized")
        self.exit_err_msg = "Quitting without binning fiducial set..."
        # data processing objects and variables
        #self.chunk_size = 100000 # not needed for 10k event sets
        self.n_evts = 10000 # adjust as needed for larger fiducial sets
        #self.alpha_code = 1.00002004e+09 # not needed
        #self.triton_code = 1.00001003e+09 # not needed
        self.neutron_code = 2112
        self.positron_code = -11
        
        # Data Directories
        self.data_dir = os.path.normpath("/home/jack/RATPAC2/ratpac-setup/ratpac/output/nulat/fiducial")
        self.output_file = os.path.join(self.data_dir, "nulat_fiducial_set.json")
        
        # Grid dimensions (5x5x5)
        self.imax = 5
        self.jmax = 5
        self.kmax = 5
	
        # Calculate Offsets to map position to index 0-4
        # Total span calculation from create_NuLat_geo.py
        total_span = 5 * self.step_size - self.gap_size
        self.offset = -total_span / 2.0 + self.cube_size / 2.0
	# Detector Geometry Parameters (Must match simulation)
        self.inch = 25.4
        self.cube_size = 2.5 * self.inch      # 63.5 mm
        self.gap_size = 0.001 * self.inch     # 0.0254 mm
        self.step_size = self.cube_size + self.gap_size

        # Analysis Parameters
        self.angles = range(0, 360) # 0 to 359 degrees
        self.results = []
        return
    ################################################################
    # Method to find cube index --- line 70
    ################################################################
    def get_cube_index(self, pos):
        """
        Maps a continuous position (mm) to a cube index (int).
        Returns the nearest integer index after rounding to nearest integer to handle floating point jitter near centers.
        """
        # Formula: index = (pos - offset) / step
        idx = round((pos - self.offset) / self.step_size)
        return int(idx)
    ################################################################
    # Method to process data files --- line 81
    ################################################################
    def processRefData(self):
        """
        Reads the ASCII files for each angle, filters for Coincident (Prompt+Delayed) pairs,
        and computes the binning matrix.
        """
        print(f"Processing data from: {self.data_dir}")
        
        for angle in self.angles:
            filename = os.path.join(self.data_dir, f"neutrons_angle_{angle}.txt")
            
            if not os.path.exists(filename):
                if self.debug and angle % 10 == 0: 
                    print(f"Angle {angle}: File not found, skipping.")
                continue

            if self.debug and angle % 45 == 0:
                print(f"Processing Angle {angle}...")

            try:
                # 1. Load Data
                # Columns defined by readCombined.C and awk cleanup
                # Expected: Row trackPDG trackProcess mcx mcy mcz mcpdg trackPosX trackPosY trackPosZ trackTime ...
                col_names = ["Row", "trackPDG", "trackProcess", 
                             "mcx", "mcy", "mcz", "mcpdg", 
                             "trackPosX", "trackPosY", "trackPosZ", "trackTime", 
                             "trackMomX", "trackMomY", "trackMomZ", "trackKE"]
                
                # Using python engine separator None to auto-detect whitespace
                df = pd.read_csv(filename, sep=r'\s+', names=col_names, comment='#', engine='python')
                
                # 2. Identify Prompt Events (Positron Annihilation)
                # PDG -11 (e+) and process "annihil"
                prompts = df[(df['trackPDG'] == -11) & (df['trackProcess'] == 'annihil')]
                
                # 3. Identify Delayed Events (Neutron Capture)
                # Process "nCapture". In undoped PVT, this is n + p -> d + gamma.
                # We simply look for the neutron track ending in capture.
                captures = df[df['trackProcess'] == 'nCapture']
                
                # 4. Match Prompt and Delayed by Event ID
                merged = pd.merge(prompts, captures, on='evid', suffixes=('_p', '_d'))
                
                if merged.empty:
                    if self.debug: print(f"Angle {angle}: No coincident pairs found.")
                    continue
                
                # 5. Map Positions to Indices (0 to 4)
                # Prompt
                merged['pi'] = merged['trackPosX_p'].apply(self.get_cube_index)
                merged['pj'] = merged['trackPosY_p'].apply(self.get_cube_index)
                merged['pk'] = merged['trackPosZ_p'].apply(self.get_cube_index)
                
                # Delayed
                merged['di'] = merged['trackPosX_d'].apply(self.get_cube_index)
                merged['dj'] = merged['trackPosY_d'].apply(self.get_cube_index)
                merged['dk'] = merged['trackPosZ_d'].apply(self.get_cube_index)
                
                # 6. Filter: Keep only events where both vertices are inside the 5x5x5 grid
                valid_mask = (
                    (merged['pi'].between(0, 4)) & (merged['pj'].between(0, 4)) & (merged['pk'].between(0, 4)) &
                    (merged['di'].between(0, 4)) & (merged['dj'].between(0, 4)) & (merged['dk'].between(0, 4))
                )
                valid_events = merged[valid_mask].copy()
                
                # 7. Calculate Relative Displacement Indices (Delayed - Prompt)
                # Range: -4 to +4
                valid_events['ri'] = valid_events['di'] - valid_events['pi']
                valid_events['rj'] = valid_events['dj'] - valid_events['pj']
                valid_events['rk'] = valid_events['dk'] - valid_events['pk']
                
                # 8. Create Binning Matrix (Histogram)
                # Dimensions: 9x9x9 (covering shifts -4 to +4)
                # Index mapping: matrix_index = relative_index + 4
                histogram = np.zeros((9, 9, 9), dtype=int)
                
                # Iterate and fill (vectorized approach possible, but loop is safe)
                for _, row in valid_events.iterrows():
                    ix = int(row['ri'] + 4)
                    iy = int(row['rj'] + 4)
                    iz = int(row['rk'] + 4)
                    if 0 <= ix < 9 and 0 <= iy < 9 and 0 <= iz < 9:
                        histogram[ix, iy, iz] += 1
                
                total_counts = int(valid_events.shape[0])
                
                # Flatten for JSON storage
                matrix_flat = histogram.flatten().tolist()
                
                entry = {
                    "angle": angle,
                    "captures": total_counts,
                    "matrix": matrix_flat
                }
                self.results.append(entry)

            except Exception as e:
                print(f"Error processing angle {angle}: {e}")
    ################################################################
    # define binning function --- line 181
    ################################################################
    def createBinningMatrices(self):
        """
        Writes the processed results to a JSON file.
        """
        if not self.results:
            print("No results to save.")
            return

        with open(self.output_file, 'w') as f:
            json.dump(self.results, f, indent=4)
        
        print(f"Successfully saved fiducial set to: {self.output_file}")
        print(f"Total angles processed: {len(self.results)}")

if __name__ == "__main__":
    cnfs = createNuLatFiducialSet()
    cnfs.processRefData()
    cnfs.createBinningMatrices()

